{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zx8J-JCo93YD",
        "outputId": "79fc3c34-6026-4453-dfb6-b2d9d2618e92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from Torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from Torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from Torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from Torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from Torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from Torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->Torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->Torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->Torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->Torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install Torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MVMdbZGg9Vpr"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import DataFrame\n",
        "from sklearn import metrics\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch as T\n",
        "device = T.device(\"cpu\")\n",
        "\n",
        "from sklearn.preprocessing import normalize, StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.backend import log, mean\n",
        "from keras.layers import Dense, Flatten, Activation, Dropout, Input\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import BinaryFocalCrossentropy\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Tmmb60UHPrG",
        "outputId": "4154625c-627c-42b0-f58a-17ef1cdcc177"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/My Drive/IRES UCy/Colab Notebooks/PU Learning\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "%cd '/content/drive/My Drive/IRES UCy/Colab Notebooks/PU Learning/'\n",
        "#will need to modify for wherever your data set is stored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KfjUC6gQIAxH"
      },
      "outputs": [],
      "source": [
        "class SolarDataset(T.utils.data.Dataset):\n",
        " #label\tDCArrayOutput_W_\t    Vmp\t         Imp\t  CellTemperature_C_\tPlaneOfArrayIrradiance_W_m_2_   Fill Factor\t    Gamma       \t  Pmp\t         Voc \t      Isc\n",
        " #-2\t  3772.327\t     36.33172302\t  1.095984553   \t16.107\t                281.111               2.595129269\t  0.141648698\t  39.81900722\t28.80097715\t3.587915457\n",
        " #-2\t  4715.409\t     36.33172302\t  1.369980764\t    16.107\t                281.111\t              4.669694852  \t0.177060882 \t49.77376167\t44.33885631\t5.242090076\n",
        " #........\n",
        " # [0]        [1]         [2]             [3]           [4]                     [5]                     [6]             [7]           [8]         [9]         [10]\n",
        "\n",
        "  def __init__(self, fn, tot_num_pos, tot_num_unl):\n",
        "    self.rnd = np.random.RandomState(1)\n",
        "\n",
        "    self.tot_num_pos = tot_num_pos  # number positive in data\n",
        "    self.tot_num_unl = tot_num_unl  # num unlabeled in data\n",
        "\n",
        "    pos_x_lst = []; pos_y_lst = []  # lists of numpy vectors\n",
        "    unl_x_lst = []; unl_y_lst = []\n",
        "\n",
        "    ln = 0  # line number (not including comments)\n",
        "    j = 0  # counter for unlabeleds\n",
        "\n",
        "    self.unl_idx_to_line_num = dict()\n",
        "    # key = idx of an unlabeled item in memory,\n",
        "    # val = corresponding line number in src data file\n",
        "\n",
        "    fin = open(fn, \"r\")  # read into four lists of arrays\n",
        "    for line in fin:\n",
        "      line = line.strip()\n",
        "      if line.startswith(\"#\"): continue\n",
        "\n",
        "      arr = np.fromstring(line, sep=\"\\t\", dtype=np.float32)\n",
        "      if arr[0] == 1:\n",
        "        pos_x = arr[[1,2,3,4,5,6,7,8,9,10]]\n",
        "        pos_y = 1  # always 1 but allows multi-class\n",
        "        pos_x_lst.append(pos_x)\n",
        "        pos_y_lst.append(pos_y)\n",
        "      elif arr[0] == -2:  # unlabeled\n",
        "        unl_x = arr[[1,2,3,4,5,6,7,8,9,10]]\n",
        "        unl_y = 0  # treat unlabeleds as negative (0)\n",
        "        unl_x_lst.append(unl_x)\n",
        "        unl_y_lst.append(unl_y)\n",
        "        self.unl_idx_to_line_num[j] = ln\n",
        "        j +=1\n",
        "      else:\n",
        "        print(\"Fatal: unknown label encountered in file\")\n",
        "\n",
        "      ln += 1  # only data lines\n",
        "\n",
        "    fin.close()\n",
        " \n",
        "    # data actual storage in 4 tensor-arrays\n",
        "    self.train_x_pos = T.tensor(pos_x_lst, dtype=T.float32) # predictors for positives\n",
        "    self.train_y_pos = T.tensor(pos_y_lst, dtype=T.float32).reshape(-1,1) # labels for positives (1s)\n",
        "    self.train_x_unl = T.tensor(unl_x_lst, dtype=T.float32) # predictors for unlabels\n",
        "    self.train_y_unl = T.tensor(unl_y_lst, dtype=T.float32).reshape(-1,1) # labels for unlabeleds (0s)\n",
        "\n",
        "    self.num_pos_unl = 2 * tot_num_pos  # num items in virtual ds\n",
        "\n",
        "    # set up indices of active and inactive unlabeled items\n",
        "    all_unl_indices = np.arange(tot_num_unl)  # 180 indices\n",
        "    self.rnd.shuffle(all_unl_indices)\n",
        "    self.p = all_unl_indices[0 : tot_num_pos]  # 20 active unlabled\n",
        "    self.q = all_unl_indices[tot_num_pos : tot_num_unl]  # inactive\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.num_pos_unl  # virtual ds size \n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if idx < self.tot_num_pos:  # small index = pos = fetch direct\n",
        "      return (self.train_x_pos[idx], self.train_y_pos[idx])\n",
        "    else:   # large index = an unlabeled = map index\n",
        "      ofset = idx - self.tot_num_pos\n",
        "      ii = self.p[ofset]  # index of active unlabeled item\n",
        "      return (self.train_x_unl[ii], self.train_y_unl[ii])\n",
        "\n",
        "  def reinit(self):  # get (20) different unlabeled items\n",
        "    all_unl_indices = np.arange(self.tot_num_unl)\n",
        "    self.rnd.shuffle(all_unl_indices)\n",
        "    self.p = all_unl_indices[0 : self.tot_num_pos] \n",
        "    self.q = all_unl_indices[self.tot_num_pos : self.tot_num_unl]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "R3DyM2l-KfhN"
      },
      "outputs": [],
      "source": [
        "class Net(T.nn.Module):\n",
        "  # binary classifier for Solar data\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.hid1 = T.nn.Linear(10, 30)  # 10-(10-10)-1\n",
        "    self.hid2 = T.nn.Linear(30, 20)\n",
        "    self.hid3 = T.nn.Linear(20, 10)\n",
        "    self.oupt = T.nn.Linear(10, 1)\n",
        "\n",
        "    T.nn.init.xavier_uniform_(self.hid1.weight) \n",
        "    T.nn.init.zeros_(self.hid1.bias)\n",
        "    T.nn.init.xavier_uniform_(self.hid2.weight) \n",
        "    T.nn.init.zeros_(self.hid2.bias)\n",
        "    T.nn.init.xavier_uniform_(self.hid3.weight) \n",
        "    T.nn.init.zeros_(self.hid3.bias)\n",
        "    T.nn.init.xavier_uniform_(self.oupt.weight) \n",
        "    T.nn.init.zeros_(self.oupt.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    z = T.tanh(self.hid1(x))\n",
        "    z = T.tanh(self.hid2(z))\n",
        "    z = T.tanh(self.hid3(z))\n",
        "    z = T.sigmoid(self.oupt(z))  # see BCELoss() below\n",
        "    return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CU1P1QxQLG3x"
      },
      "outputs": [],
      "source": [
        "def train(net, ds, bs, me, le, lr, verbose):\n",
        "  # NN, dataset, batch_size, max_epochs,\n",
        "  # log_every, learn_rate. optimizer and loss hard-coded.\n",
        "  net.train()\n",
        "  data_ldr = T.utils.data.DataLoader(ds, batch_size=bs, shuffle=True)\n",
        "  loss_func = T.nn.BCELoss()  # assumes sigmoid activation\n",
        "  opt = T.optim.Adam(net.parameters(), lr=lr)\n",
        "  scheduler = T.optim.lr_scheduler.StepLR(opt, step_size=100, gamma=0.025) #decreases learning rate by gamma every step_size\n",
        "  for epoch in range(0, me):\n",
        "    epoch_loss = 0.0\n",
        "    for (batch_idx, batch) in enumerate(data_ldr):\n",
        "      X = batch[0]  # inputs\n",
        "      Y = batch[1]  # targets\n",
        "\n",
        "      opt.zero_grad()                # prepare gradients\n",
        "      oupt = net(X)                  # compute output/target\n",
        "      loss_val = loss_func(oupt, Y)  # a tensor\n",
        "      epoch_loss += loss_val.item()  # accumulate for display\n",
        "      loss_val.backward()            # compute gradients\n",
        "      opt.step()                     # update weights\n",
        "      scheduler.step()               # update learning rate\n",
        "\n",
        "      if epoch % le == 0 and verbose == True:\n",
        "        print(\" epoch = %4d   loss = %0.4f\" % (epoch, epoch_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "X-HEKlkNLZRK"
      },
      "outputs": [],
      "source": [
        "def truth_of_line(ln):\n",
        "  # actual label for 0-based line number of PUL file\n",
        "  if ln % 2 == 0: return 0  # files set up this way \n",
        "  else: return 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def confusion_matrix_ascii(TN, FP, FN, TP):\n",
        "  cm = np.array([[TN, FP], [FN, TP]])\n",
        "  print(\"Predicted  0   1\")\n",
        "  print(\"True 0    \" + str(cm[:,0]))\n",
        "  print(\"True 1    \" + str(cm[:,1]))"
      ],
      "metadata": {
        "id": "87MsnoTFh-9b"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fercWAWaL3d0",
        "outputId": "54beddd1-322d-472d-8ab8-e81a5164accb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Solar PUL using PyTorch \n",
            "\n",
            "Creating dynamic Solar train Dataset \n",
            "Dataset has 20 positive and 180 unlabeled \n",
            "\n",
            "Creating 10-(10-10)-1 binary NN classifier \n",
            "\n",
            "Setting training parameters \n",
            "\n",
            "batch size = 32\n",
            "initial lrn_rate = 0.05 \n",
            "max_epochs = 800\n",
            "loss function = BCELoss() \n",
            "optimizer = Adam \n",
            "\n",
            "Training model 0 of 8 epoch =    0   loss = 0.8801\n",
            " epoch =    0   loss = 1.6575\n",
            " epoch =  100   loss = 0.1583\n",
            " epoch =  100   loss = 0.9753\n",
            " epoch =  200   loss = 0.2249\n",
            " epoch =  200   loss = 0.7735\n",
            " epoch =  300   loss = 0.2684\n",
            " epoch =  300   loss = 0.6430\n",
            " epoch =  400   loss = 0.2230\n",
            " epoch =  400   loss = 0.7792\n",
            " epoch =  500   loss = 0.3392\n",
            " epoch =  500   loss = 0.4305\n",
            " epoch =  600   loss = 0.2606\n",
            " epoch =  600   loss = 0.6663\n",
            " epoch =  700   loss = 0.3050\n",
            " epoch =  700   loss = 0.5332\n",
            "  Done. Scoring inactive unlabeled items \n",
            "Training model 1 of 8 epoch =    0   loss = 0.5494\n",
            " epoch =    0   loss = 0.8152\n",
            " epoch =  100   loss = 0.2552\n",
            " epoch =  100   loss = 0.3525\n",
            " epoch =  200   loss = 0.1391\n",
            " epoch =  200   loss = 0.6995\n",
            " epoch =  300   loss = 0.2388\n",
            " epoch =  300   loss = 0.4004\n",
            " epoch =  400   loss = 0.2547\n",
            " epoch =  400   loss = 0.3529\n",
            " epoch =  500   loss = 0.1620\n",
            " epoch =  500   loss = 0.6308\n",
            " epoch =  600   loss = 0.2067\n",
            " epoch =  600   loss = 0.4969\n",
            " epoch =  700   loss = 0.2109\n",
            " epoch =  700   loss = 0.4843\n",
            "  Done. Scoring inactive unlabeled items \n",
            "Training model 2 of 8 epoch =    0   loss = 0.2108\n",
            " epoch =    0   loss = 0.5826\n",
            " epoch =  100   loss = 0.1171\n",
            " epoch =  100   loss = 0.1418\n",
            " epoch =  200   loss = 0.1184\n",
            " epoch =  200   loss = 0.1376\n",
            " epoch =  300   loss = 0.1176\n",
            " epoch =  300   loss = 0.1400\n",
            " epoch =  400   loss = 0.1187\n",
            " epoch =  400   loss = 0.1367\n",
            " epoch =  500   loss = 0.0201\n",
            " epoch =  500   loss = 0.4326\n",
            " epoch =  600   loss = 0.1180\n",
            " epoch =  600   loss = 0.1389\n",
            " epoch =  700   loss = 0.1199\n",
            " epoch =  700   loss = 0.1334\n",
            "  Done. Scoring inactive unlabeled items \n",
            "Training model 3 of 8 epoch =    0   loss = 0.4093\n",
            " epoch =    0   loss = 1.1220\n",
            " epoch =  100   loss = 0.2399\n",
            " epoch =  100   loss = 0.3588\n",
            " epoch =  200   loss = 0.2381\n",
            " epoch =  200   loss = 0.3635\n",
            " epoch =  300   loss = 0.1993\n",
            " epoch =  300   loss = 0.4800\n",
            " epoch =  400   loss = 0.2401\n",
            " epoch =  400   loss = 0.3575\n",
            " epoch =  500   loss = 0.1646\n",
            " epoch =  500   loss = 0.5842\n",
            " epoch =  600   loss = 0.2401\n",
            " epoch =  600   loss = 0.3576\n",
            " epoch =  700   loss = 0.2210\n",
            " epoch =  700   loss = 0.4148\n",
            "  Done. Scoring inactive unlabeled items \n",
            "Training model 4 of 8 epoch =    0   loss = 1.1856\n",
            " epoch =    0   loss = 1.4258\n",
            " epoch =  100   loss = 0.1729\n",
            " epoch =  100   loss = 0.5255\n",
            " epoch =  200   loss = 0.1728\n",
            " epoch =  200   loss = 0.5234\n",
            " epoch =  300   loss = 0.2527\n",
            " epoch =  300   loss = 0.2838\n",
            " epoch =  400   loss = 0.1699\n",
            " epoch =  400   loss = 0.5321\n",
            " epoch =  500   loss = 0.1838\n",
            " epoch =  500   loss = 0.4905\n",
            " epoch =  600   loss = 0.2051\n",
            " epoch =  600   loss = 0.4266\n",
            " epoch =  700   loss = 0.1940\n",
            " epoch =  700   loss = 0.4598\n",
            "  Done. Scoring inactive unlabeled items \n",
            "Training model 5 of 8 epoch =    0   loss = 0.4745\n",
            " epoch =    0   loss = 0.6415\n",
            " epoch =  100   loss = 0.1831\n",
            " epoch =  100   loss = 0.3763\n",
            " epoch =  200   loss = 0.1864\n",
            " epoch =  200   loss = 0.3662\n",
            " epoch =  300   loss = 0.1984\n",
            " epoch =  300   loss = 0.3301\n",
            " epoch =  400   loss = 0.1685\n",
            " epoch =  400   loss = 0.4197\n",
            " epoch =  500   loss = 0.1996\n",
            " epoch =  500   loss = 0.3264\n",
            " epoch =  600   loss = 0.1148\n",
            " epoch =  600   loss = 0.5809\n",
            " epoch =  700   loss = 0.2152\n",
            " epoch =  700   loss = 0.2798\n",
            "  Done. Scoring inactive unlabeled items \n",
            "Training model 6 of 8 epoch =    0   loss = 0.4198\n",
            " epoch =    0   loss = 1.2429\n",
            " epoch =  100   loss = 0.2014\n",
            " epoch =  100   loss = 0.6428\n",
            " epoch =  200   loss = 0.2631\n",
            " epoch =  200   loss = 0.4572\n",
            " epoch =  300   loss = 0.2592\n",
            " epoch =  300   loss = 0.4692\n",
            " epoch =  400   loss = 0.2637\n",
            " epoch =  400   loss = 0.4555\n",
            " epoch =  500   loss = 0.3009\n",
            " epoch =  500   loss = 0.3440\n",
            " epoch =  600   loss = 0.2606\n",
            " epoch =  600   loss = 0.4649\n",
            " epoch =  700   loss = 0.2585\n",
            " epoch =  700   loss = 0.4711\n",
            "  Done. Scoring inactive unlabeled items \n",
            "Training model 7 of 8 epoch =    0   loss = 0.3347\n",
            " epoch =    0   loss = 0.4620\n",
            " epoch =  100   loss = 0.1242\n",
            " epoch =  100   loss = 0.2194\n",
            " epoch =  200   loss = 0.1475\n",
            " epoch =  200   loss = 0.1496\n",
            " epoch =  300   loss = 0.1354\n",
            " epoch =  300   loss = 0.1858\n",
            " epoch =  400   loss = 0.1243\n",
            " epoch =  400   loss = 0.2192\n",
            " epoch =  500   loss = 0.1123\n",
            " epoch =  500   loss = 0.2552\n",
            " epoch =  600   loss = 0.1477\n",
            " epoch =  600   loss = 0.1490\n",
            " epoch =  700   loss = 0.1158\n",
            " epoch =  700   loss = 0.2446\n",
            "  Done. Scoring inactive unlabeled items \n",
            "\n",
            "Guessing 0 or 1 for unlabeled items \n",
            "pseudo-prob thresholds: 0.45  0.55 \n",
            "\n",
            "---------------\n",
            "\n",
            "num labels guessed = 179\n",
            "num correct guessed labels = 153\n",
            "num wrong guessed labels   = 26\n",
            "pct of unlabeled items guessed = 0.9944 \n",
            "accuracy of guessed items = 0.8547 \n",
            "\n",
            "---------------\n",
            "\n",
            "confusion matrix : \n",
            "Predicted  0   1\n",
            "True 0    [100  26]\n",
            "True 1    [ 0 53]\n"
          ]
        }
      ],
      "source": [
        "# 0. get started\n",
        "print(\"\\nSolar PUL using PyTorch \\n\")\n",
        "T.manual_seed(1)\n",
        "np.random.seed(1)\n",
        "\n",
        "# 1. create data objects and standardize\n",
        "print(\"Creating dynamic Solar train Dataset \")\n",
        "print(\"Dataset has 20 positive and 180 unlabeled \")\n",
        "df = pd.read_csv(\"asu_solar_dataset_PUL_200.csv\")\n",
        "df_x = df[['DCArrayOutput_W_', 'Vmp', 'Imp', 'CellTemperature_C_', 'PlaneOfArrayIrradiance_W_m_2_', 'Fill Factor', 'Gamma', 'Pmp', 'Voc', 'Isc']]\n",
        "df[['DCArrayOutput_W_', 'Vmp', 'Imp', 'CellTemperature_C_', 'PlaneOfArrayIrradiance_W_m_2_', 'Fill Factor', 'Gamma', 'Pmp', 'Voc', 'Isc']] = (df_x-df_x.mean())/df_x.std()\n",
        "df_csv = df.to_csv('train_file_200.txt', sep='\\t', index=False, header=False)\n",
        "train_file = 'train_file_200.txt'\n",
        "train_ds = SolarDataset(train_file, 20, 180)\n",
        "\n",
        "# 2. create neural network\n",
        "print(\"\\nCreating 10-(10-10)-1 binary NN classifier \")\n",
        "net = Net().to(device)\n",
        "\n",
        "# 3. prepare for training multiple times\n",
        "print(\"\\nSetting training parameters \\n\")\n",
        "bat_size = 32\n",
        "lrn_rate = 0.05\n",
        "max_epochs = 800\n",
        "ep_log_interval = 100\n",
        "\n",
        "print(\"batch size = \" + str(bat_size))\n",
        "print(\"initial lrn_rate = %0.2f \" % lrn_rate)\n",
        "print(\"max_epochs = \" + str(max_epochs))\n",
        "print(\"loss function = BCELoss() \")\n",
        "print(\"optimizer = Adam \\n\")\n",
        "\n",
        "# track number times each inactive unlabeled is evaluated\n",
        "# accumulate sum of p-values from each evaluation\n",
        "eval_counts = np.zeros(180, dtype=np.int64)\n",
        "eval_sums = np.zeros(180, dtype=np.float32)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "# 4. accumulate p-values for inactive items after session\n",
        "num_trials = 8  # number times to train on a subset\n",
        "for trial in range(num_trials):\n",
        "  print(\"Training model \" + str(trial) + \" of \" + str(num_trials), end=\"\")\n",
        "  train(net, train_ds, bat_size, max_epochs, ep_log_interval, lrn_rate, verbose=True) \n",
        "\n",
        "  print(\"  Done. Scoring inactive unlabeled items \")\n",
        "  net.eval()\n",
        "  for i in train_ds.q:  # idxs of inactive unlabeleds\n",
        "    x = train_ds.train_x_unl[i]  # predictors\n",
        "    with T.no_grad():\n",
        "      p = net(x)          # between 0.0 and 1.0\n",
        "    eval_counts[i] += 1\n",
        "    eval_sums[i] += p.item()\n",
        "\n",
        "  train_ds.reinit()   # get different unlabeleds\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "# 5. guess 0 or 1 labels for unlabeled items\n",
        "print(\"\\nGuessing 0 or 1 for unlabeled items \")\n",
        "\n",
        "lo = 0.45; hi = 0.55   # tune for accuracy vs. quantity\n",
        "\n",
        "# there is more information about positives so\n",
        "# to label an unknown as positive you need a higher \n",
        "# p-value criterion.\n",
        "\n",
        "print(\"pseudo-prob thresholds: %0.2f  %0.2f \" % (lo, hi))\n",
        "\n",
        "#initialize counting variables\n",
        "TN, TP, FP, FN = 0, 0, 0, 0\n",
        "num_correct = 0; num_wrong = 0\n",
        "for i in range(180):  # process each unlabeled data item\n",
        "  ln = train_ds.unl_idx_to_line_num[i]  # line num in PUL file\n",
        "\n",
        "  if eval_counts[i] == 0:\n",
        "    print(\"Fatal: Never evaluated this unlabeled item \")\n",
        "    input()\n",
        "  else:\n",
        "    avg_p = (eval_sums[i] * 1.0) / eval_counts[i]\n",
        "    if avg_p >= lo and avg_p <= hi:  #too close to 0.5\n",
        "      pass\n",
        "    elif avg_p < lo and truth_of_line(ln) == 0: #predicted 0, actual 0\n",
        "      num_correct += 1\n",
        "      TN += 1\n",
        "    elif avg_p > hi and truth_of_line(ln) == 1: #predicted 1, actual 1\n",
        "      num_correct += 1\n",
        "      TP += 1\n",
        "    elif avg_p < lo and truth_of_line(ln) == 1: #predicted 0, actual 1\n",
        "      num_wrong += 1\n",
        "      FN += 1\n",
        "    elif avg_p > hi and truth_of_line(ln) == 0: #predicted 1, actual 0\n",
        "      num_wrong += 1\n",
        "      FP += 1\n",
        "    else :\n",
        "      print(\"error in guessing, please restart\")\n",
        "\n",
        "\n",
        "print(\"\\n---------------\\n\")\n",
        "num_guessed = num_correct + num_wrong\n",
        "print(\"num labels guessed = \" + str(num_guessed))\n",
        "print(\"num correct guessed labels = \" + str(num_correct))\n",
        "print(\"num wrong guessed labels   = \" + str(num_wrong))\n",
        "acc = (1.0 * num_correct) / (num_correct + num_wrong) \n",
        "pct = (1.0 * (num_correct + num_wrong)) / 180\n",
        "\n",
        "print(\"pct of unlabeled items guessed = %0.4f \" % pct)\n",
        "print(\"accuracy of guessed items = %0.4f \" % acc)\n",
        "\n",
        "print(\"\\n---------------\\n\")\n",
        "print(\"confusion matrix : \")\n",
        "confusion_matrix_ascii(TN, FP, FN, TP)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}